# Retrospective Sprint 4 :eyeglasses: 

## What We Wanted to Get Better At - Did it Happen?
> Working with Bruce and friends on a long term project / formalize a long term project.

We totally did this :)
> Be better o'clock lumberjacks (better time logging)

Congratulations Stuart--nice job with improving on this. This is still an area of opportunity for growth for @burkhardtr.

> We need to revisit repo setup (e.g. `pip install`ing local modules)

We didn't end up having time to address this. This didn't cause us any grief this sprint, however.

## What We Wanted to Stop Doing - Did we stop?

![f3c](uploads/e3afddae7c656569fb024f5dea4ed7ca/f3c.gif)
> Stop _not_ logging time

See above, most of the team made good progress here!
> No more electronics failures

Unfortunately, Joe's SD card for his pi experienced a failure. The crime is currently being attributed to the family ghost.

> Don't let the pipeline fail for an extended period of time
Good job here!

## What we should continue doing - How We Did
> Continue not logging negative time.

This happened once, but it was quickly fixed. None of the issues we had propagated, as we've learned to fix them very easily.

## What to Start Doing
* Regularly communicate PBI progress on shared PBI's (this isn't a problem right now, but the upcoming sprint will require close collaboration for team members).
* Get in touch with Matt about the siscloud startup issues/general inquiries about the architecture
* Attempt short (~5m) team bonding activity during weekly Monday meeting, to improve morale 
* Recurring meeting invites (@flemingg)

## What to Stop Doing
* Thinking it's 2020, and logging time for last winter.
* Stop planning entire sprint in 1 meeting (not a systemic issue, really just an issue in this sprint due to the spike nature of PBI's. This is listed more as a caution to upcoming sprints)
* Having hardware issues (continued)

## What We Should Continue Doing - New/Current
* Having Monday sync meetings
* Good team communication--when Joe's SD card bummed out, the team was active in helping him recover, and with ongoing remote issues, multiple people pitched in to troubleshoot the newly exposed (though not yet functional) web app on port 3001.